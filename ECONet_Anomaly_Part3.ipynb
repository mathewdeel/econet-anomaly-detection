{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Template: Phase 3\n",
    "\n",
    "Below are some concrete steps that you can take while doing your analysis for phase 3. This guide isn't \"one size fit all\" so you will probably not do everything listed. But it still serves as a good \"pipeline\" for how to do data analysis.\n",
    "\n",
    "If you do engage in a step, you should clearly mention it in the notebook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from random import randrange\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Either must import notebook or repaste methods probably\n",
    "# from G3_FinalProject_Part2 import apply_feature_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods from Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized SMOTE algorithm as defined by Chawla et al. in the report \"SMOTE: Synthetic Minority Over-sampling Technique\".\n",
    "# https://arxiv.org/pdf/1106.1813.pdf\n",
    "\n",
    "# Functional version\n",
    "def SMOTE(minority_df, smote_percent, k):\n",
    "    \"\"\"\n",
    "    Input:  minority_df   - dataframe consisting ONLY of minority class samples\n",
    "            smote_percent - percent of samples to SMOTE as a decimal\n",
    "            k             - number of nearest neighbors to use for SMOTE\n",
    "    Output: array of synthetic minority class samples of size floor( minority_df.length * smote_percent )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of objects (rows) in minority_df\n",
    "    obj_count = minority_df.shape[0]\n",
    "    # Number of attributes (columns) in minority_df\n",
    "    attr_count = minority_df.shape[1]\n",
    "    # Keeps count of number of synthetic samples generated\n",
    "    synth_count = 0\n",
    "    \n",
    "    # If smote_percent is less than 100% (1), shuffle minority_df's rows because only some objects will be sampled\n",
    "    if smote_percent < 1:\n",
    "        # This code shuffles the dataframe's rows in-place and resets the indices \n",
    "        minority_df = minority_df.sample(frac=1).reset_index(drop=True)\n",
    "        # Set number of objects to SMOTE\n",
    "        obj_count = math.floor(obj_count * smote_percent)\n",
    "        # Set smote_percent to 100% \n",
    "        smote_percent = 1\n",
    "        \n",
    "    # Else smote_percent is assumed to be in multiples of 100%\n",
    "    smote_percent = math.floor( smote_percent )\n",
    "    \n",
    "    # List of synthetic samples to output, will be converted to dataframe before output\n",
    "    synth_list = [[0 for x in range(attr_count)] for y in range(obj_count*smote_percent)] \n",
    "\n",
    "    # Inner function to generate the synthetic samples\n",
    "    def Populate(smote_percent, i, nnarray):\n",
    "        nonlocal synth_count\n",
    "        while smote_percent != 0:\n",
    "            # Choose a random number between 0 and k-1 and assign it to nn.\n",
    "            # This chooses on of the KNNs of i\n",
    "            nn = randrange(k)\n",
    "            new_nnarray = nnarray[0]\n",
    "            # Create the synthetic attributes\n",
    "            for attr in range(attr_count):      \n",
    "                dif = minority_df.iloc[new_nnarray[nn], attr] - minority_df.iloc[i, attr]\n",
    "                gap = random.random() \n",
    "                synth_list[synth_count][attr] = minority_df.iloc[i, attr] + gap * dif\n",
    "            synth_count += 1\n",
    "            smote_percent -= 1\n",
    "        return \n",
    "    \n",
    "    # Compute the KNNs for each of minority_df's samples and use them to populate synth_list\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(minority_df)\n",
    "    for i in range(obj_count):\n",
    "        # List of i's KNNs as indices\n",
    "        nnarray = neigh.kneighbors([minority_df.iloc[i].values.flatten().tolist()], return_distance=False)\n",
    "        # Populate the list of synthetic samples\n",
    "        Populate(smote_percent, i, nnarray)\n",
    "    \n",
    "    # Return the dataframe of synthetic samples\n",
    "    \n",
    "    return pd.DataFrame(synth_list, columns = minority_df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialized SMOTE function to apply to the ECONet dataset that\n",
    "# automatically performs smote and manages the \"Station\" and \"measure\" data\n",
    "# This version ignores the \"station\"-\"measure\" correlation. Only works for smote_percent >= 1\n",
    "# TODO Update this description\n",
    "\n",
    "def ECONet_SMOTE_IgnoreStationCorrelation(X_train, Y_train, smote_percent, k ):\n",
    "    \"\"\"\n",
    "    Input: The original X_train and Y_train training dataset\n",
    "           smote_percent - percent of samples to SMOTE as a decimal\n",
    "           k             - number of nearest neighbors to use for SMOTE\n",
    "    Output: A new training dataset with sampling applied (same columns, different rows)\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, copy the dataframes so they are not modified in place\n",
    "    X_train, Y_train = X_train.copy(\"deep=True\"), Y_train.copy(\"deep=True\")\n",
    "    \n",
    "    # Now add Y back to X to more easily get the minority class\n",
    "    df = pd.concat([X_train, Y_train], axis=1)\n",
    "    \n",
    "    # Get a new dataframe that has only the minority class\n",
    "    minority_df = df[df[\"target\"] == True]\n",
    "    \n",
    "    # Now remove the \"target\" row from the minority dataframe, it is useless now that all rows are \"True\"\n",
    "    minority_df = minority_df[[\"Station\", \"Ob\", \"value\", \"measure\", \"R_flag\", \"I_flag\",\"Z_flag\", \"B_flag\"]]\n",
    "    \n",
    "    # Now, loop through every measure to produce synthetic dataframes for each\n",
    "    # then merge them together into one big synthetic dataframe. This is necessary because SMOTE does not work\n",
    "    # with categorical variables like measure\n",
    "    \n",
    "    # Big synthetic dataframe to output\n",
    "    synthetic_df = []\n",
    "    \n",
    "    # List of unique \"measure\" values in the dataframe\n",
    "    measure_values = minority_df[\"measure\"].unique()\n",
    "    \n",
    "    # Loop through each measure\n",
    "    for measure in measure_values:\n",
    "        # Get a sub-dataframe that has just this measure \n",
    "        sub_df = minority_df[minority_df[\"measure\"] == measure]\n",
    "\n",
    "        # Continue thorugh the loop if this dataframe has no rows\n",
    "        if sub_df.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Remove the \"measure\" attributes from the dataframe so we can SMOTE it\n",
    "        sub_df = sub_df[[\"Station\", \"Ob\", \"value\", \"R_flag\", \"I_flag\",\"Z_flag\", \"B_flag\"]]\n",
    "        \n",
    "        # Get the \"Station\" column and drop it from the sub-dataframe\n",
    "        stations = sub_df[\"Station\"]\n",
    "        sub_df = sub_df[[\"Ob\", \"value\", \"R_flag\", \"I_flag\",\"Z_flag\", \"B_flag\"]]\n",
    "        \n",
    "        # Update \"Station\" column by multiplying column by smote_percent\n",
    "        stations = stations.loc[stations.index.repeat(smote_percent)].reset_index(drop=True)\n",
    "\n",
    "        # If k is greater than the number of rows in df, set it to number of rows in df\n",
    "        # TODO Fix this? Explain?\n",
    "        if(k > sub_df.shape[0]):\n",
    "            k_new = sub_df.shape[0]\n",
    "        else:\n",
    "            k_new = k\n",
    "\n",
    "        # Smote the dataframe to get synthetic samples that resemble this combination    \n",
    "        synthetic_sub_df = SMOTE(sub_df, smote_percent, k_new)\n",
    "\n",
    "        # Add the \"Station\" and \"measure\" attributes back to the dataframe\n",
    "        synthetic_sub_df.insert(0, 'Station', stations)\n",
    "        synthetic_sub_df.insert(3, 'measure', measure)\n",
    "\n",
    "        # Round the flag values so they are discrete. \n",
    "        # TODO This approach should be updated but its fine for now\n",
    "        synthetic_sub_df['R_flag'] = synthetic_sub_df['R_flag'].map(round)\n",
    "        synthetic_sub_df['I_flag'] = synthetic_sub_df['I_flag'].map(round)\n",
    "        synthetic_sub_df['Z_flag'] = synthetic_sub_df['Z_flag'].map(round)\n",
    "        synthetic_sub_df['B_flag'] = synthetic_sub_df['B_flag'].map(round)\n",
    "\n",
    "        # Finally, add this new synthetic sub-dataframe to the main synthetic dataframe to output\n",
    "        synthetic_df.extend(synthetic_sub_df.values.tolist())\n",
    "            \n",
    "    # Now, add \"target\" column to the synthetic dataframe before output\n",
    "    synthetic_df = pd.DataFrame(synthetic_df, columns = X_train.columns.values.tolist())\n",
    "    synthetic_df.insert(4, 'target', True)\n",
    "    \n",
    "    # Output the new synthetic dataframe\n",
    "    return (synthetic_df.sort_values(\"Station\")).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Input: The original X_train and Y_train training dataset\n",
    "    Output: A new training dataset with sampling applied (same columns, different rows)\n",
    "    \"\"\"\n",
    "    # For example, undersample the majority class, or oversample the minority class.\n",
    "    synth_df = ECONet_SMOTE_IgnoreStationCorrelation(X_train, Y_train, 30, 4)\n",
    "    \n",
    "    # Split the synthetic dataframe into X and Y\n",
    "    synth_df_copy = synth_df.copy(\"deep=True\")\n",
    "\n",
    "    X_synth_train, Y_synth_train = synth_df_copy.drop(\"target\", axis=1), synth_df_copy[\"target\"]\n",
    "\n",
    "    # Combine the synthetic set with the original training set\n",
    "    X_train_copy, Y_train_copy = X_train.copy(\"deep=True\"), Y_train.copy(\"deep=True\")\n",
    "\n",
    "    X_train_copy, Y_train_copy = pd.concat([X_train_copy, X_synth_train], ignore_index=True), pd.concat([Y_train_copy, Y_synth_train], ignore_index=True)\n",
    "    \n",
    "    return (X_train_copy, Y_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_transformation(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Input: The original X_train and X_test feature sets.\n",
    "    Output: The transformed X_train and X_test feature sets.\n",
    "    \"\"\"\n",
    "    catStation = ['AURO', 'BAHA', 'BALD', 'BEAR', 'BUCK', 'BURN', 'CAST', 'CHAP', 'CLA2', 'CLAY', 'CLIN', 'DURH', \n",
    "                  'FLET', 'FRYI', 'GOLD', 'HAML', 'JACK', 'JEFF', 'KINS', 'LAKE', 'LAUR', 'LEWS', 'LILE', 'MITC', \n",
    "                  'NCAT', 'NEWL', 'OXFO', 'PLYM', 'REED', 'REID', 'ROCK', 'SALI', 'SASS', 'SILR', 'SPIN', 'SPRU', \n",
    "                  'TAYL', 'UNCA', 'WAYN', 'WHIT', 'WILD', 'WILL', 'WINE']\n",
    "    \n",
    "    catMeasure = ['temp_wxt', 'temp_hmp', 'rh_wxt', 'rh_hmp', 'ws10', 'wd10', 'gust10', 'precip', 'impact',\n",
    "                   'pres', 'par', 'sr', 'st', 'sm', 'temp10', 'ws02', 'wd02', 'gust02', 'ws06', 'wd06', 'gust06',\n",
    "                   'leafwetness', 'blackglobetemp']\n",
    "    \n",
    "    X_train['Station'] = X_train['Station'].astype('category').cat.set_categories(catStation)\n",
    "    X_train['Station'] = X_train['Station'].astype('category').cat.codes\n",
    "    X_train['measure'] = X_train['measure'].astype('category').cat.set_categories(catMeasure)\n",
    "    X_train['measure'] = X_train['measure'].astype('category').cat.codes\n",
    "\n",
    "    X_test['Station'] = X_test['Station'].astype('category').cat.set_categories(catStation)\n",
    "    X_test['Station'] = X_test['Station'].astype('category').cat.codes\n",
    "    X_test['measure'] = X_test['measure'].astype('category').cat.set_categories(catMeasure)\n",
    "    X_test['measure'] = X_test['measure'].astype('category').cat.codes\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0) Evaluation\n",
    "\n",
    "Now that you have selected your models and have trained/tuned them, it's time to see how they stack up. Some important questsion to ask:\n",
    "\n",
    "1. How did your models compare to each other\n",
    "2. In what metrics do they differ, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Comparing Models\n",
    "\n",
    "To compare your models, you can try things such as:\n",
    "\n",
    "1. Doing multiple random restarts of training/test splits (code below)\n",
    "2. Using cross-validation\n",
    "\n",
    "In your report, report back the following metrics:\n",
    "\n",
    "**Classification**\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1\n",
    "* AUC\n",
    "\n",
    "**Regression**\n",
    "* MSE\n",
    "* MAE\n",
    "* $R^2$\n",
    "\n",
    "**Sample Evaluation Code**: Here is some sample code for the evaluation procedure. **You do not need to use the sample code if you feel that it wouldn't work with your pipeline, but you can use it as inspiration. It lacks any sort of feature transformation or sampling procedure, so you would have to implement that yourself.** It runs a set number of trials using different splits, and returns back a dataframe, where each row represents a single random evaluation. It has 3 columns.\n",
    "    \n",
    "Model: The name of the model being evaluated\n",
    "\n",
    "Evaluation: The name of the evaluation (e.g. acc, precision, MSE)\n",
    "\n",
    "Score: The score of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\"\"\"\n",
    "Evaluates a classification model\n",
    "\"\"\"\n",
    "def evaluate_classification(model,x_test_ev,y_test_ev):\n",
    "    predictions = model.predict(x_test_ev)\n",
    "    \n",
    "    acc = accuracy_score(y_test_ev,predictions)\n",
    "    \n",
    "    # Depending on the type of classification you are doing (e.g. multiclass vs binary)\n",
    "    # Make sure to change the \"average\" param depending on what you need\n",
    "    prec = precision_score(y_test_ev,predictions,average=\"macro\")\n",
    "    recall = recall_score(y_test_ev,predictions,average=\"macro\")\n",
    "    f1 = f1_score(y_test_ev,predictions,average=\"macro\")\n",
    "    # Make sure to change/edit the `multi_class` of the ROC if you're doing multiclass\n",
    "    auc = roc_auc_score(y_test_ev,predictions)\n",
    "    \n",
    "    return {\"acc\":acc,\"precision\":prec,\"recall\":recall,\"f1\":f1,\"auc\":auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\"\"\"\n",
    "Evaluates regression using MAE,MSE, and R^2\n",
    "\"\"\"\n",
    "def evaluate_regression(model,x_test_ev,y_test_ev):\n",
    "    predictions = model.predict(x_test_ev)\n",
    "    mae = mean_absolute_error(y_test_ev,predictions)\n",
    "    mse = mean_squared_error(y_test_ev,predictions)\n",
    "    r2 = r2_score(y_test_ev,predictions)\n",
    "    return {\"mae\":mae,\"mse\":mse,\"r2\":r2}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains and evaluates a single model on a random train/test split\n",
    "\"\"\"\n",
    "def evaluate_random(model,X_train,y_train,X_test,y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # Switch this out with `evaluate_regression` if you're doing a regression problem\n",
    "    evals = evaluate_classification(model,X_test,y_test)\n",
    "    #evals = evaluate_regression(model,X_test,y_test)\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    X: Your features\n",
    "    y: Your target\n",
    "    models: A list of the models that you are evaluating\n",
    "    n_trials (opt): The number of random trials\n",
    "    \n",
    "Output:\n",
    "    A dataframe with three colums and len(models)*n_trials*(number of evaluation metrics) rows.\n",
    "    Each row represents a single random evaluation.\n",
    "    \n",
    "    Model: The name of the model being evaluated\n",
    "    Evaluation: The name of the evaluation (e.g. acc, precision, MSE)\n",
    "    Score: The score of the evaluation\n",
    "\"\"\"\n",
    "def get_scores(X,y,models,n_trials=5):\n",
    "    \n",
    "    data = {\n",
    "        \"model\": [],\n",
    "        \"evaluation\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    \n",
    "    for n in range(n_trials):\n",
    "        for model in models:\n",
    "            # Put in special sampling methods\n",
    "            \n",
    "            X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "            # Put in feature scaling here\n",
    "            # MinMaxScaler()\n",
    "            \n",
    "            scores = evaluate_random(model,X_train,y_train,X_test,y_test)\n",
    "            \n",
    "            for key in scores:\n",
    "                data[\"model\"].append(str(model))\n",
    "                data[\"evaluation\"].append(key)\n",
    "                data[\"score\"].append(scores[key])\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>acc</td>\n",
       "      <td>0.902098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.897727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.879934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>f1</td>\n",
       "      <td>0.887831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier(n_neighbors=3)</td>\n",
       "      <td>auc</td>\n",
       "      <td>0.879934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model evaluation     score\n",
       "0  KNeighborsClassifier(n_neighbors=3)        acc  0.902098\n",
       "1  KNeighborsClassifier(n_neighbors=3)  precision  0.897727\n",
       "2  KNeighborsClassifier(n_neighbors=3)     recall  0.879934\n",
       "3  KNeighborsClassifier(n_neighbors=3)         f1  0.887831\n",
       "4  KNeighborsClassifier(n_neighbors=3)        auc  0.879934"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of getting classification scores\n",
    "# (See \"Follow\" doc for how to do it with regression)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = load_breast_cancer(return_X_y=True,as_frame=True)\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "get_scores(X,y,[neigh],5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a boxplot of the different random runs (see Follow document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "train = pd.read_csv(\"transformed_train.csv\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( train.drop(\"target\", axis=1), train[\"target\"], test_size=0.25, random_state=random_seed)\n",
    "\n",
    "X_train_smoted, Y_train_smoted = sample_data(X_train, Y_train)\n",
    "X_train_tansformed, X_test_tansformed = apply_feature_transformation(X_train_smoted, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually test model for whatever desired metrics to get boxplot data\n",
    "model = KNeighborsClassifier(n_neighbors=2, weights=\"distance\", algorithm=\"auto\", p=1).fit(X_train_tansformed, Y_train_smoted)\n",
    "predictions = model.predict(X_test_tansformed)\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters were tuned in part 2 using a combination of\n",
    "# automatic and manual hyperparamater testing for models/SMOTE\n",
    "model1 = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model2 = KNeighborsClassifier(n_neighbors=2, weights=\"distance\", algorithm=\"auto\", p=1)\n",
    "model3 = SVC(max_iter=5)\n",
    "#The sample method did not work well with our pipeline\n",
    "#get_scores(X,Y,[model1, model2, model3],3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table with the average evaluation scores of each metric for each model\n",
    "**Bold** the best result for each.\n",
    "\n",
    "Here's an example for a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | LinReg | SVR   | MLP   |\n",
    "|-----|--------|-------|-------|\n",
    "| **MAE** | 3.061  | 4.143 | **2.71**  |\n",
    "| **MSE** | 22.09  | 42.42 | **15.37** |\n",
    "| **$R^2$** | 0.684  | 0.394 | **0.780** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the actual training/test data\n",
    "train = pd.read_csv(\"transformed_train.csv\")\n",
    "test = pd.read_csv(\"transformed_test.csv\")\n",
    "X_train = train.drop(\"target\", axis=1)\n",
    "Y_train = train[\"target\"]\n",
    "X_test = test.drop(\"target\", axis=1)\n",
    "Y_test = test[\"target\"]\n",
    "X_train_smoted, Y_train_smoted = sample_data(X_train, Y_train)\n",
    "X_train_tansformed, X_test_tansformed = apply_feature_transformation(X_train_smoted, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 0: Decision Tree on real test\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=20, min_impurity_decrease=0.0004).fit(X_train_smoted, Y_train_smoted)\n",
    "test_pred = model.predict_proba(X_test_tansformed)\n",
    "pd.DataFrame(test_pred[:,1], columns=['target']).to_csv('predictions.csv', index=False)\n",
    "print(classification_report(Y_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0) Technical Retrospective\n",
    "\n",
    "Now that you have your final model, go back and look at how your decisions impacted the results. This can take many forms, here are some ideas:\n",
    "\n",
    "* Which of your decisions were helpful? With your best model:\n",
    "    * Compare the results of the model with an without your feature selection\n",
    "    * Compare the results with and without feature engineering\n",
    "    * Compare if your sampling method made a difference\n",
    "    \n",
    "    \n",
    "* Why did your model do well?\n",
    "    * If your model is interpretable, discuss feature importance (e.g. decision tree splits, coefficients of linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The biggest decision in model training that was helpful was implementing SMOTE. Oversampling saw a major increase in the overall performance of our model. Also, encoding the categorical features using static arrays was not only great for performance, but vastly sped up the speed of our program's execution time. Lastly, a simple decision like standardizing our features caused the performance of our models to shoot up drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0) Writeup \n",
    "\n",
    "Now it is time to reflect upon your work and tie up your report. The goal of this project was to get you familiar with doing a data science problem from scratch on a custom dataset. First, write some conclusions about your model. Then, consider how it could be used in practice. Finally, write about your experiences and what you learned from this project.\n",
    "\n",
    "Use the following questions as inspiration.\n",
    "\n",
    "1. How could we use this model in practice?\n",
    "2. Would you trust the model to make decisions?\n",
    "3. What are the limitations of the model?\n",
    "4. what are alternative approaches you could try in the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We could use this to predict erroneous weather measures from the ECONet stations with a higher level of precision and recall than the current quality control system. This model could then be used instead to minimize the amount of readings people have to manually review.\n",
    "2. We could probably trust our model to make decisions, because the results have been fairly high across the board showing high levels of average precision in both k-NN and decision trees. It probably would not be a universal answer to ECONet erroneous data predicting at a usable level,  but it is a step in the right direction.\n",
    "3. A major limitation of our model is the execution time. Oversamping takes over half an hour which drastically slows down the speed of development and testing. Other than that, our model is fairly flexible with the data it can use as long as categorical features are encoded properly and there are not unknown unique categories between training and testing sets.\n",
    "4. Alternative approaches could firstly include more types of oversampling which could potentially generate more accurate and useful synthetic samples. We could also try evaluating other models that we didn't look into this time such as neural networks, which could potentially find better connections between features like \"measure\" and \"value\" to improve predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
